{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cfcda0-b453-4dd0-ac04-8f9ee09cab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a97644-a354-4400-93ee-6297349c5534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nans:-\\nFilter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\\nSome common techniques of Filter methods are as follows:\\n-->Information Gain\\n-->Chi-square Test\\n-->Fisher's Score\\n-->Missing Value Ratio\\n1)Information Gain: Information gain determines the reduction in entropy while transforming the dataset. It can be used\\nas a feature selection technique by calculating the information gain of each variable with respect to the target variable.\\n2)Chi-square Test: Chi-square test is a technique to determine the relationship between the categorical variables. \\nThe chi-square value is calculated between each feature and the target variable, and the desired number of features with the best chi-square value is selected.\\n3)Fisher's Score:\\n\\nFisher's score is one of the popular supervised technique of features selection. \\nIt returns the rank of the variable on the fisher's criteria in descending order. Then we can select the variables with a large fisher's score.\\n4)Missing Value Ratio::\\nThe value of the missing value ratio can be used for evaluating the feature set against the threshold value.\\nThe formula for obtaining the missing value ratio is the number of missing values in each column divided by the total number of observations. \\nThe variable is having more than the threshold value can be dropped.\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ans:-\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "Some common techniques of Filter methods are as follows:\n",
    "-->Information Gain\n",
    "-->Chi-square Test\n",
    "-->Fisher's Score\n",
    "-->Missing Value Ratio\n",
    "1)Information Gain: Information gain determines the reduction in entropy while transforming the dataset. It can be used\n",
    "as a feature selection technique by calculating the information gain of each variable with respect to the target variable.\n",
    "2)Chi-square Test: Chi-square test is a technique to determine the relationship between the categorical variables. \n",
    "The chi-square value is calculated between each feature and the target variable, and the desired number of features with the best chi-square value is selected.\n",
    "3)Fisher's Score:\n",
    "\n",
    "Fisher's score is one of the popular supervised technique of features selection. \n",
    "It returns the rank of the variable on the fisher's criteria in descending order. Then we can select the variables with a large fisher's score.\n",
    "4)Missing Value Ratio::\n",
    "The value of the missing value ratio can be used for evaluating the feature set against the threshold value.\n",
    "The formula for obtaining the missing value ratio is the number of missing values in each column divided by the total number of observations. \n",
    "The variable is having more than the threshold value can be dropped.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dbf93e-4ffb-4ef3-85be-57a3db78919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24bb92e-b341-40be-bed1-05e451350167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nans:-\\n1)ilter:-\\n-->The significance of features are measured by their association with a dependent variable\\n-->It is faster\\n-->Statistical ways are applied to evaluate a subset of features\\n-->It might fail to find the best subset of features\\n2)wrapper method:-\\n-->The best subset of the features is measured by a relly training model on it.\\n-->very expensive and mathematical\\n-->It uses cross validation\\n-->It can always supply the best subset of features\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ans:-\n",
    "1)ilter:-\n",
    "-->The significance of features are measured by their association with a dependent variable\n",
    "-->It is faster\n",
    "-->Statistical ways are applied to evaluate a subset of features\n",
    "-->It might fail to find the best subset of features\n",
    "2)wrapper method:-\n",
    "-->The best subset of the features is measured by a relly training model on it.\n",
    "-->very expensive and mathematical\n",
    "-->It uses cross validation\n",
    "-->It can always supply the best subset of features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45581cb-913e-41cd-b8e7-ea4521827cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27435ba3-ddc0-4c9f-b4d4-7400f9dc2c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nans:-\\nEmbedded Methods:-\\nSome techniques of embedded methods are:\\n-->Regularization- Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model. \\n-->Random Forest Importance - Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ans:-\n",
    "Embedded Methods:-\n",
    "Some techniques of embedded methods are:\n",
    "-->Regularization- Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model. \n",
    "-->Random Forest Importance - Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a3d4d5-4828-4fd9-b817-6307d5ef9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57925bb8-4bb9-49c8-b60d-09c3af59b748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nans:-\\n1)No interaction with classification for feature selection.\\n2)Mostly ignores feature dependencies and considers eah feature \\nseparately in case of univariate techniques,which may lead to low computational \\nperformance as compared to other technique of feature selection.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ans:-\n",
    "1)No interaction with classification for feature selection.\n",
    "2)Mostly ignores feature dependencies and considers eah feature \n",
    "separately in case of univariate techniques,which may lead to low computational \n",
    "performance as compared to other technique of feature selection.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e932495-de99-4167-ab10-889be6282dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "#selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac21414-9e5e-4a9c-95a1-e5d8a547f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ans-when we have to check relationship between each feature and the output we use filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58973219-5ad5-450b-8343-0a89d6ed178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\\nYou are unsure of which features to include in the model because the dataset contains several different\\nones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab77fc0-7f20-4478-85e7-4cd862aac621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUsing feature engineering I will manage the Nan datas.Then I will run different filter techniques like Pearson's Correlation\\nfor continous data containing coloumn and Anova for categorical to see the relation between coloumns .Then the coloum having least relation with\\ntarget coloumn will get removed and the coloumn having most relation with target coloum will be fed to the model for further processing.\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using feature engineering I will manage the Nan datas.Then I will run different filter techniques like Pearson's Correlation\n",
    "for continous data containing coloumn and Anova for categorical to see the relation between coloumns .Then the coloum having least relation with\n",
    "target coloumn will get removed and the coloumn having most relation with target coloum will be fed to the model for further processing.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e5e3a0-b720-4cda-8f82-9ca2ffa7333c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\\nmany features, including player statistics and team rankings. Explain how you would use the Embedded\\nmethod to select the most relevant features for the model.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddcc7b4-1ffa-4d54-8efa-47516eb3b40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nans-using feature engineering I will try to manage the missing values then I will try to find the outliers and check its percentage \\nout of the total data.If it is less then I will remove it else let it stay there.\\nAfter it i will use different techniques to find the correlation between different coloumns.\\nThe coloumn having most correlation with the target coloumn will be choosen\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ans-using feature engineering I will try to manage the missing values then I will try to find the outliers and check its percentage \n",
    "out of the total data.If it is less then I will remove it else let it stay there.\n",
    "After it i will use different techniques to find the correlation between different coloumns.\n",
    "The coloumn having most correlation with the target coloumn will be choosen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67392eee-340d-496d-af35-e9ad3c9a8f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ8. You are working on a project to predict the price of a house based on its features, such as size, location,\\nand age. You have a limited number of features, and you want to ensure that you select the most important\\nones for the model. Explain how you would use the Wrapper method to select the best set of features for the\\npredictor.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44937328-2843-4896-ad43-0091b188972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here first I will use Forward selection  in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "Then I will use Backward Elimination in this process\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
